{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理：\n",
    "# 1. 视频转化为图片\n",
    "# 2. 图片segment分割\n",
    "# 3. 图片聚类得到label\n",
    "# 4. 文本预处理\n",
    "# 5. 最终数据合并    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ 1. 视频转化为图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有的csv合并在一起。\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 设置文件夹路径和输出文件名\n",
    "folder_path = '/data1/dxw_data/llm/redbook_final/data/red_02/good'  # 替换为你的文件夹路径\n",
    "# good文件夹里面是:\n",
    "# 02_post_data_2024-06-16.csv\n",
    "# 02_post_data_2024-06-17.csv\n",
    "# 02_post_data_2024-06-18.csv\n",
    "# 02_post_data_2024-06-19.csv\n",
    "# 02_post_data_2024-06-20.csv\n",
    "# 02_post_data_2024-06-21.csv\n",
    "# 02_post_data_2024-06-22.cs\n",
    "# 02_post_data_2024-06-23.csv\n",
    "# 02_post_data_2024-06-24.csv\n",
    "# 02_post_data_2024-06-25.csv\n",
    "# 02_post_data_2024-06-26.csv\n",
    "# 02_post_data_2024-06-27.csv\n",
    "# 02_post_data_2024-06-28.csv\n",
    "# 02_post_data_2024-06-29.csv\n",
    "# 02_post_data_2024-06-30.csv\n",
    "# 02_post_data_2024-07-01.csv\n",
    "# 02_post_data_2024-07-02.csv\n",
    "# 02_post_data_2024-07-03.csv\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data/red_02/merged_good_raw.csv'  # 替换为你想要输出的文件名\n",
    "\n",
    "# 创建一个空的列表来存储所有数据\n",
    "all_data_frames = []\n",
    "\n",
    "# 遍历文件夹中的所有CSV文件\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # 读取CSV文件并追加到列表中\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_data_frames.append(df)\n",
    "\n",
    "# 将所有数据框合并到一个大的数据框中\n",
    "merged_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "\n",
    "# 将合并后的数据保存到一个新的CSV文件中\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'所有文件已合并到 {output_file}')\n",
    "\n",
    "#! 里面有些行看起来有问题，但是其实都是对的，不用管。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 50/50 [00:00<00:00, 57.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件处理完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取 合并额csv_path 文件中的数据，并根据文件层次和目录读取对应的 path_filex 中的所有 png 和 mp4 文件，然后将它们重新命名并保存到 output_file_path 中。\n",
    "# 按照日期排序\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置文件路径\n",
    "csv_path = \"/home/disk1/red_disk1/test/poster_test_50.csv\"  # 请替换为你的CSV文件路径\n",
    "path_filex = \"/home/disk1/red_disk1/test/data\"  # 请替换为你的path_filex目录路径\n",
    "output_file_path = \"/home/disk1/red_disk1/test/video_img_all\"  # 请替换为你的输出文件路径\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 遍历每一行数据，加入进度条\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing files\"):\n",
    "    poster_id = row['poster_id']\n",
    "    post_id = row['post_id']\n",
    "    post_date = row['post_date']\n",
    "    \n",
    "    # 检查并修改post_date\n",
    "    if post_date.startswith('2023-') or post_date.startswith('2024-'):\n",
    "        post_date = post_date[5:]  # 删减掉前面的年份部分\n",
    "    \n",
    "    # 获取子文件夹路径\n",
    "    subfolder_path = os.path.join(path_filex, str(poster_id), str(post_id))\n",
    "    \n",
    "    # 检查子文件夹是否存在\n",
    "    if os.path.exists(subfolder_path):\n",
    "        # 遍历子文件夹中的文件\n",
    "        for file_name in os.listdir(subfolder_path):\n",
    "            # 获取文件的完整路径\n",
    "            file_path = os.path.join(subfolder_path, file_name)\n",
    "            \n",
    "            # 仅处理png和mp4文件\n",
    "            if file_name.endswith('.png') or file_name.endswith('.mp4'):\n",
    "                # 新文件名\n",
    "                new_file_name = f\"{post_date}_{poster_id}_{post_id}_{file_name}\"\n",
    "                new_file_path = os.path.join(output_file_path, new_file_name)\n",
    "                \n",
    "                # 创建输出文件夹路径（如果不存在）\n",
    "                os.makedirs(output_file_path, exist_ok=True)\n",
    "                \n",
    "                # 复制并重命名文件到输出文件夹\n",
    "                shutil.copy(file_path, new_file_path)\n",
    "                \n",
    "print(\"文件处理完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 69/69 [00:10<00:00,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件处理完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 将视频也变成对应的png图片。\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置文件路径\n",
    "input_file_path = \"/home/disk1/red_disk1/test/video_img_all\"\n",
    "output_file_path = \"/home/disk1/red_disk1/test/video_img_all_png\"\n",
    "\n",
    "# 创建输出文件夹路径（如果不存在）\n",
    "os.makedirs(output_file_path, exist_ok=True)\n",
    "\n",
    "# 遍历输入文件夹中的文件\n",
    "for file_name in tqdm(os.listdir(input_file_path), desc=\"Processing files\"):\n",
    "    input_file = os.path.join(input_file_path, file_name)\n",
    "    \n",
    "    # 处理mp4文件\n",
    "    if file_name.endswith('.mp4'):\n",
    "        # 读取视频文件\n",
    "        video_capture = cv2.VideoCapture(input_file)\n",
    "        fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count / fps\n",
    "        \n",
    "        # 获取视频在1/4, 2/4, 3/4时刻的帧\n",
    "        for i in range(1, 4):\n",
    "            frame_time = duration * i / 4\n",
    "            video_capture.set(cv2.CAP_PROP_POS_MSEC, frame_time * 1000)\n",
    "            success, frame = video_capture.read()\n",
    "            if success:\n",
    "                # 保存帧为图片\n",
    "                output_image_name = f\"{file_name[:-4]}_{i}.png\"\n",
    "                output_image_path = os.path.join(output_file_path, output_image_name)\n",
    "                cv2.imwrite(output_image_path, frame)\n",
    "        \n",
    "        # 释放视频捕获对象\n",
    "        video_capture.release()\n",
    "    \n",
    "    # 处理png文件\n",
    "    elif file_name.endswith('.png'):\n",
    "        output_image_path = os.path.join(output_file_path, file_name)\n",
    "        shutil.copy(input_file, output_image_path)\n",
    "\n",
    "print(\"文件处理完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################### 2. 图片segment分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用segment模型，将video_img_all_png变为segment_data，得到mask图片。在命令行运行下列指令\n",
    "\n",
    "# 去github下载安装：https://github.com/cxgincsu/SemanticGuidedHumanMatting\n",
    "\n",
    "\n",
    "# bash 运行SemanticGuidedHumanMatting,\n",
    "cd /home/disk1/red_disk1/SemanticGuidedHumanMatting\n",
    "\n",
    "   CUDA_VISIBLE_DEVICES=1 python test_image.py \\\n",
    "       --images-dir \"/home/disk1/red_disk1/test/video_img_all_png\" \\\n",
    "       --result-dir \"/home/disk1/red_disk1/test/segment_data\" \\\n",
    "       --pretrained-weight ./pretrained/SGHM-ResNet50.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并掩码mask图片，得到训练用的imge文件夹\n",
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # 导入tqdm库\n",
    "\n",
    "def apply_mask(image, mask):\n",
    "    \"\"\"\n",
    "    将掩码应用到图片上，保留掩码区域内的图片部分\n",
    "    \"\"\"\n",
    "    image_np = np.array(image)\n",
    "    mask_np = np.array(mask)\n",
    "    \n",
    "    # 增加掩码对比度\n",
    "    enhancer = ImageEnhance.Contrast(mask)\n",
    "    mask = enhancer.enhance(2.0)\n",
    "    \n",
    "    mask_np = np.array(mask)\n",
    "    \n",
    "    # 将掩码二值化，保留白色部分（255）\n",
    "    mask_binary = (mask_np > 128).astype(np.uint8)  # 使用阈值化处理掩码\n",
    "    \n",
    "    # 应用掩码，保留白色区域\n",
    "    masked_image = image_np * mask_binary[:, :, np.newaxis]\n",
    "    \n",
    "    # 确保掩码区域透明处理\n",
    "    alpha_channel = mask_binary * 255\n",
    "    masked_image = np.dstack((masked_image, alpha_channel))\n",
    "    \n",
    "    return Image.fromarray(masked_image.astype(np.uint8))\n",
    "\n",
    "def merge_images(image_dir, mask_dir, output_dir):\n",
    "    \"\"\"\n",
    "    合并图片和掩码，并保存到新的文件夹\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "    mask_files = [f for f in os.listdir(mask_dir) if f.endswith('.png')]\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        mask_path = os.path.join(mask_dir, image_file)\n",
    "        \n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"掩码缺失: {mask_path}\")\n",
    "            continue\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # 假设掩码是灰度图\n",
    "        \n",
    "        masked_image = apply_mask(image, mask)\n",
    "        output_path = os.path.join(output_dir, image_file)\n",
    "        masked_image.save(output_path, 'PNG')  # 保存为PNG格式以保留透明度\n",
    "        # print(f\"图片已合并并保存到 {output_path}！\")\n",
    "\n",
    "# 定义图片、掩码和输出文件夹路径\n",
    "image_dir = r'/data1/dxw_data/llm/redbook_final/data2/video_img_all_png'  # 替换为你的正常图片文件夹路径\n",
    "mask_dir = r'/data1/dxw_data/llm/redbook_final/data2/segment_data'  # 替换为你的掩码图片文件夹路径\n",
    "output_dir = r'/data1/dxw_data/llm/redbook_final/data2/combined_seg_img'  # 替换为你的输出文件夹路径\n",
    "\n",
    "merge_images(image_dir, mask_dir, output_dir)\n",
    "\n",
    "\n",
    "\n",
    "########## 至此，得到图片mask数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【可视化分割不准确的特殊情况】\n",
    "\n",
    "1. 增加对比度增强：通过 ImageEnhance.Contrast 增加掩码的对比度，使得掩码更明显。\n",
    "2. 可视化特殊情况：通过 visualize_special_cases 函数可视化并保存分割不准确的情况。\n",
    "3. 添加特殊情况检测：在 merge_images 函数中，添加了对掩码区域过小或过大的情况检测，并将这些特殊情况保存到 special_cases_dir 目录中。\n",
    "4. 使用 tqdm 进行进度显示：方便查看处理进度。\n",
    "5. 使用 matplotlib 进行可视化：可以更加灵活地展示图片和掩码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_mask(image, mask, threshold=128):\n",
    "    \"\"\"\n",
    "    将掩码应用到图片上，保留掩码区域内的图片部分\n",
    "    \"\"\"\n",
    "    image_np = np.array(image)\n",
    "    mask_np = np.array(mask)\n",
    "\n",
    "    # 增加掩码对比度\n",
    "    enhancer = ImageEnhance.Contrast(mask)\n",
    "    mask = enhancer.enhance(2.0)\n",
    "\n",
    "    mask_np = np.array(mask)\n",
    "\n",
    "    # 将掩码二值化，保留白色部分（255）\n",
    "    mask_binary = (mask_np > threshold).astype(np.uint8)  # 使用阈值化处理掩码\n",
    "\n",
    "    # 应用掩码，保留白色区域\n",
    "    masked_image = image_np * mask_binary[:, :, np.newaxis]\n",
    "\n",
    "    # 确保掩码区域透明处理\n",
    "    alpha_channel = mask_binary * 255\n",
    "    masked_image = np.dstack((masked_image, alpha_channel))\n",
    "\n",
    "    return Image.fromarray(masked_image.astype(np.uint8)), mask_binary\n",
    "\n",
    "def visualize_special_cases(image, mask, output_path, issue_type):\n",
    "    \"\"\"\n",
    "    可视化并保存分割不准确的情况\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[1].imshow(mask, cmap='gray')\n",
    "    ax[1].set_title(f'Issue: {issue_type}')\n",
    "    plt.suptitle('Special Case Detected')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def merge_images(image_dir, mask_dir, output_dir, special_cases_dir, threshold=128):\n",
    "    \"\"\"\n",
    "    合并图片和掩码，并保存到新的文件夹，同时可视化和保存特殊情况\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    if not os.path.exists(special_cases_dir):\n",
    "        os.makedirs(special_cases_dir)\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "    mask_files = [f for f in os.listdir(mask_dir) if f.endswith('.png')]\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        mask_path = os.path.join(mask_dir, image_file)\n",
    "        \n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"掩码缺失: {mask_path}\")\n",
    "            continue\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # 假设掩码是灰度图\n",
    "        \n",
    "        masked_image, mask_binary = apply_mask(image, mask, threshold)\n",
    "        output_path = os.path.join(output_dir, image_file)\n",
    "        masked_image.save(output_path, 'PNG')  # 保存为PNG格式以保留透明度\n",
    "\n",
    "        # 检查特殊情况并可视化\n",
    "        if np.sum(mask_binary) < 0.1 * mask_binary.size:  # 例如：掩码区域过小\n",
    "            special_case_path = os.path.join(special_cases_dir, f\"small_mask_{image_file}\")\n",
    "            visualize_special_cases(image, mask, special_case_path, \"Small Mask Area\")\n",
    "        elif np.sum(mask_binary) > 0.9 * mask_binary.size:  # 例如：掩码区域过大\n",
    "            special_case_path = os.path.join(special_cases_dir, f\"large_mask_{image_file}\")\n",
    "            visualize_special_cases(image, mask, special_case_path, \"Large Mask Area\")\n",
    "\n",
    "# 定义图片、掩码和输出文件夹路径\n",
    "image_dir = r'/data1/dxw_data/llm/redbook_final/data2/video_img_all_png'  # 替换为你的正常图片文件夹路径\n",
    "mask_dir = r'/data1/dxw_data/llm/redbook_final/data2/segment_data'  # 替换为你的掩码图片文件夹路径\n",
    "output_dir = r'/data1/dxw_data/llm/redbook_final/data2/combined_seg_img'  # 替换为你的输出文件夹路径\n",
    "special_cases_dir = r'/data1/dxw_data/llm/redbook_final/data2/special_cases'  # 替换为你的特殊情况输出文件夹路径\n",
    "\n",
    "merge_images(image_dir, mask_dir, output_dir, special_cases_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ 3. 图片聚类得到label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定聚类数量的imagebind的算法，开始给图片分配标签\n",
    "# 需要从https://github.com/facebookresearch/ImageBind?tab=readme-ov-file中下载：imagebind的预训练 保存到/.checkpoints：  \n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "from imagebind import data\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "input_folder = '/data1/dxw_data/llm/redbook_final/data3/combined_seg_img' \n",
    "output_folder = '/data1/dxw_data/llm/redbook_final/data3/output_cluster_imagebind3'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda:4\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load image paths\n",
    "image_paths = [os.path.join(input_folder, fname) for fname in os.listdir(input_folder) if fname.endswith('.png')]\n",
    "\n",
    "# Function to load and transform a batch of images\n",
    "def load_images_batch(image_paths_batch):\n",
    "    images = [transform(Image.open(path).convert('RGB')) for path in image_paths_batch]\n",
    "    return torch.stack(images).to(device)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "\n",
    "# Generate embeddings in batches\n",
    "all_embeddings = []\n",
    "for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Generating embeddings\"):\n",
    "    batch_paths = image_paths[i:i + batch_size]\n",
    "    images_tensor = load_images_batch(batch_paths)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model({ModalityType.VISION: images_tensor})\n",
    "    all_embeddings.append(embeddings[ModalityType.VISION].cpu())\n",
    "    torch.cuda.empty_cache()  # Clear cache to free memory\n",
    "\n",
    "# Concatenate all embeddings\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=20, random_state=0)\n",
    "labels = kmeans.fit_predict(all_embeddings.numpy())\n",
    "\n",
    "# Save clustered images to output folders\n",
    "for idx, label in tqdm(enumerate(labels), desc=\"Saving clustered images\", total=len(labels)):\n",
    "    label_folder = os.path.join(output_folder, str(label))\n",
    "    os.makedirs(label_folder, exist_ok=True)\n",
    "    shutil.copy(image_paths[idx], os.path.join(label_folder, os.path.basename(image_paths[idx])))\n",
    "\n",
    "# Save labels to JSON\n",
    "labels_json = {os.path.basename(image_paths[idx]): int(label) for idx, label in enumerate(labels)}\n",
    "with open(os.path.join(output_folder, 'labels.json'), 'w') as f:\n",
    "    json.dump(labels_json, f)\n",
    "\n",
    "print(f'Clustering complete. Output saved to {output_folder}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到的分类label结果格式转化\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 读取labels.json文件\n",
    "labels_file = '/data1/dxw_data/llm/redbook_final/data3/output_cluster_imagebind3/labels.json'\n",
    "with open(labels_file, 'r', encoding='utf-8') as f:\n",
    "    labels_data = json.load(f)\n",
    "\n",
    "# 初始化结果列表\n",
    "results = []\n",
    "\n",
    "# 处理每个标签\n",
    "for image_name, category in labels_data.items():\n",
    "    parts = image_name.split('_')\n",
    "    poster_id = parts[1]\n",
    "    post_id = parts[2]\n",
    "    results.append({\n",
    "        'poster_id': poster_id,\n",
    "        'post_id': post_id,\n",
    "        'nums_category_img': category\n",
    "    })\n",
    "\n",
    "# 转换结果为DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data3/clustered_labels_imagebind100.csv'   # 最终训练用的结果\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Data has been successfully saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################## 4. 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "# 读取停用词表\n",
    "stopwords_file_path = '/data1/dxw_data/llm/ML/LIWC/datasets/stopwords_cn.txt'\n",
    "with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().splitlines())\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_file_path = '/data1/dxw_data/llm/redbook_final/data/red_01/merged_good_raw.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 将post_title, post_content, post_tag列的内容拼接合并为一个新的列summary\n",
    "df['summary'] = df['post_title'].astype(str) + \" \" + df['post_content'].astype(str) + \" \" + df['post_tag'].astype(str)\n",
    "\n",
    "# 定义数据清洗和过滤函数\n",
    "def clean_summary(text, stopwords):\n",
    "    # 去除HTML标签\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 转换表情符号\n",
    "    text = emoji.demojize(text)\n",
    "    # 分词\n",
    "    words = jieba.cut(text)\n",
    "    # 去除停用词\n",
    "    cleaned_text = ' '.join([word for word in words if word not in stopwords])\n",
    "    return cleaned_text\n",
    "\n",
    "# 定义同义词过滤函数\n",
    "def replace_synonyms(value):\n",
    "    value = str(value).replace('+', '')\n",
    "    value = re.sub(r'(\\d+)k', lambda m: str(int(m.group(1)) * 1000), value)\n",
    "    value = re.sub(r'(\\d+)w', lambda m: str(int(m.group(1)) * 10000), value)\n",
    "    value = value.replace('1千', '1000').replace('1万', '10000')\n",
    "    return value\n",
    "\n",
    "# 应用清洗和过滤函数到summary列\n",
    "df['summary'] = df['summary'].apply(lambda x: clean_summary(x, stopwords))\n",
    "\n",
    "# 应用同义词过滤函数到post_comments, post_like, post_collect列\n",
    "for col in ['post_comments', 'post_like', 'post_collect']:\n",
    "    df[col] = df[col].apply(replace_synonyms)\n",
    "\n",
    "# 保存结果到新的CSV文件\n",
    "cleaned_csv_file_path = '/data1/dxw_data/llm/redbook_final/data/red_01/merged_good_raw_nlpclean.csv'\n",
    "df.to_csv(cleaned_csv_file_path, index=False)\n",
    "\n",
    "print(f\"Data has been cleaned, stopwords removed, emojis converted, and synonyms standardized. The cleaned data has been saved to {cleaned_csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### 5. 最终数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 合并文本数值信息到图片中。\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "merged_df = pd.read_csv('/data1/dxw_data/llm/redbook_final/data/red_03/merged_good_raw_nlpclean.csv')\n",
    "clustered_df = pd.read_csv('/data1/dxw_data/llm/redbook_final/data3/clustered_labels_imagebind100.csv')\n",
    "\n",
    "# 选择需要的列\n",
    "selected_columns = ['poster_id', 'post_id', 'post_date', 'post_like', 'post_collect']\n",
    "\n",
    "# 从 merged_df 中提取所需列\n",
    "merged_selected_df = merged_df[selected_columns]\n",
    "\n",
    "# 合并数据框\n",
    "merged_result_df = pd.merge(clustered_df, merged_selected_df, on=['poster_id', 'post_id'], how='left')\n",
    "\n",
    "# 保存结果到新的 CSV 文件\n",
    "merged_result_df.to_csv('/data1/dxw_data/llm/redbook_final/script_next/alldata_20%.csv', index=False)\n",
    "\n",
    "print(\"数据已成功合并并保存到新的 CSV 文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以上，数据预处理步骤完成。\n",
    "# 之后请跳转label.ipynb文件，开始进行分类和回归任务的label具体划分"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
