{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw data samples: 426\n",
      "Total after 2 month data samples: 633\n",
      "Training raw data samples: 355\n",
      "Testing raw data samples: 71\n",
      "Training after 2 month data samples: 555\n",
      "Testing after 2 month data samples: 78\n",
      "Training after 2 month data samples after cleaning: 548\n",
      "Testing after 2 month data samples after cleaning: 78\n",
      "Number of training windows: 296\n",
      "Number of testing windows: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 296/296 [18:31<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed samples: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 12/12 [00:42<00:00,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed samples: 3\n",
      "Number of training samples: 137\n",
      "Number of test samples: 3\n",
      "Number of batches in train_loader: 137\n",
      "Number of batches in test_loader: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure CUDA_LAUNCH_BLOCKING is set for better debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Define file paths\n",
    "data_dir = \"/data1/dxw_data/llm/redbook_final/script_next/\"\n",
    "rawdata_path = os.path.join(data_dir, \"rawdata_20%.csv\")\n",
    "after2monthdata_path = os.path.join(data_dir, \"after2monthdata_20%_with_trend.csv\")\n",
    "image_dir = os.path.join(data_dir, \"data_img_20%\")\n",
    "\n",
    "# Check if files and directories exist\n",
    "assert os.path.exists(rawdata_path), f\"File not found: {rawdata_path}\"\n",
    "assert os.path.exists(after2monthdata_path), f\"File not found: {after2monthdata_path}\"\n",
    "assert os.path.exists(image_dir), f\"Directory not found: {image_dir}\"\n",
    "\n",
    "# Read CSV files\n",
    "rawdata = pd.read_csv(rawdata_path)\n",
    "after2monthdata = pd.read_csv(after2monthdata_path)\n",
    "\n",
    "# Convert date columns to standard format\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "rawdata['post_date'] = rawdata['post_date'].apply(parse_date)\n",
    "rawdata = rawdata.dropna(subset=['post_date'])\n",
    "\n",
    "# Function to randomly sample a subset of the data\n",
    "def get_subset_indices(data, fraction=0.01):\n",
    "    data_size = len(data)\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(fraction * data_size))\n",
    "    return indices[:split]\n",
    "\n",
    "# Random sampling of 1/10th of the data\n",
    "subset_indices = get_subset_indices(rawdata)\n",
    "rawdata = rawdata.iloc[subset_indices]\n",
    "after2monthdata = after2monthdata[after2monthdata['post_id'].isin(rawdata['post_id'])]\n",
    "\n",
    "print(f\"Total raw data samples: {len(rawdata)}\")\n",
    "print(f\"Total after 2 month data samples: {len(after2monthdata)}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_rawdata = rawdata[(rawdata['post_date'].dt.month >= 1) & (rawdata['post_date'].dt.month <= 9)]\n",
    "test_rawdata = rawdata[(rawdata['post_date'].dt.month >= 10) & (rawdata['post_date'].dt.month >= 10)]\n",
    "\n",
    "train_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(train_rawdata['post_id'])]\n",
    "test_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(test_rawdata['post_id'])]\n",
    "\n",
    "print(f\"Training raw data samples: {len(train_rawdata)}\")\n",
    "print(f\"Testing raw data samples: {len(test_rawdata)}\")\n",
    "print(f\"Training after 2 month data samples: {len(train_after2monthdata)}\")\n",
    "print(f\"Testing after 2 month data samples: {len(test_after2monthdata)}\")\n",
    "\n",
    "# Remove non-finite values in the 'trend' column\n",
    "train_after2monthdata = train_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['trend'])\n",
    "test_after2monthdata = test_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['trend'])\n",
    "\n",
    "# Ensure that the 'trend' column has correct integer labels\n",
    "train_after2monthdata['trend'] = train_after2monthdata['trend'].astype(int)\n",
    "test_after2monthdata['trend'] = test_after2monthdata['trend'].astype(int)\n",
    "\n",
    "print(f\"Training after 2 month data samples after cleaning: {len(train_after2monthdata)}\")\n",
    "print(f\"Testing after 2 month data samples after cleaning: {len(test_after2monthdata)}\")\n",
    "\n",
    "# Define a sliding window function\n",
    "def sliding_window(data, window_size=60):\n",
    "    num_windows = len(data) - window_size + 1\n",
    "    return [data[i:i + window_size] for i in range(num_windows)]\n",
    "\n",
    "# Get sliding windows for training and testing data\n",
    "train_windows = sliding_window(train_rawdata)\n",
    "test_windows = sliding_window(test_rawdata)\n",
    "\n",
    "print(f\"Number of training windows: {len(train_windows)}\")\n",
    "print(f\"Number of testing windows: {len(test_windows)}\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, windows, after2monthdata, image_dir, transform=None, max_images=1):\n",
    "        self.windows = windows\n",
    "        self.after2monthdata = after2monthdata\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.max_images = max_images\n",
    "        self.data = self._prepare_data()\n",
    "        print(f\"Number of processed samples: {len(self.data)}\")  # Debug information\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        data = []\n",
    "        for window in tqdm(self.windows, desc=\"Processing data\"):\n",
    "            window_data = []\n",
    "            for _, row in window.iterrows():\n",
    "                poster_id = row['poster_id']\n",
    "                post_id = row['post_id']\n",
    "                image_files = [f for f in os.listdir(self.image_dir) if f\"{poster_id}_{post_id}\" in f]\n",
    "                images = []\n",
    "                for image_file in image_files[:self.max_images]:\n",
    "                    image_path = os.path.join(self.image_dir, image_file)\n",
    "                    try:\n",
    "                        image = Image.open(image_path).convert('RGB')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error opening image {image_path}: {e}\")\n",
    "                        continue\n",
    "                    if self.transform:\n",
    "                        image = self.transform(image)\n",
    "                    images.append(image)\n",
    "                while len(images) < self.max_images:\n",
    "                    images.append(torch.zeros((3, 224, 224)))\n",
    "                if images:\n",
    "                    summary = row['summary']\n",
    "                    window_data.append((summary, images))\n",
    "            if window_data:\n",
    "                last_day_post_id = window.iloc[-1]['post_id']\n",
    "                label_data = self.after2monthdata[self.after2monthdata['post_id'] == last_day_post_id]['trend']\n",
    "                if not label_data.empty:\n",
    "                    label = label_data.values[0]\n",
    "                    one_hot_label = F.one_hot(torch.tensor(label + 1), num_classes=3)  # -1, 0, 1 -> 0, 1, 2\n",
    "                    data.append((window_data, one_hot_label))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window_data, label = self.data[idx]\n",
    "        summaries, images = zip(*window_data)\n",
    "        images = [torch.stack(image_set).float() for image_set in images]\n",
    "        return summaries, torch.stack(images), label.float()\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = MultimodalDataset(train_windows, train_after2monthdata, image_dir, transform=transform)\n",
    "test_dataset = MultimodalDataset(test_windows, test_after2monthdata, image_dir, transform=transform)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageBind model loaded and set to evaluation mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████| 137/137 [02:50<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.572589163123049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 137/137 [03:12<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.3838501403883208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 137/137 [03:13<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.38307427907920016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 137/137 [03:14<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.35162049028886494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 137/137 [03:13<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.3737174421480864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 137/137 [03:14<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.3696300596760137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 137/137 [03:12<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.3618762664306555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 137/137 [03:11<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.3640773192823042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 137/137 [03:12<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.38011545704229033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 137/137 [03:11<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.38075533506535264\n",
      "Model training completed and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [00:02<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda:4\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load imagebind model\n",
    "imagebind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind_model.eval()\n",
    "imagebind_model.to(device)\n",
    "print(\"ImageBind model loaded and set to evaluation mode.\")\n",
    "\n",
    "class CrossAttentionFusionLSTM(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads):\n",
    "        super(CrossAttentionFusionLSTM, self).__init__()\n",
    "        self.text_linear = nn.Linear(text_embedding_dim, common_embedding_dim)\n",
    "        self.vision_linear = nn.Linear(vision_embedding_dim, common_embedding_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=common_embedding_dim, nhead=num_heads), num_layers=2)\n",
    "        self.lstm = nn.LSTM(common_embedding_dim, common_embedding_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(common_embedding_dim, 3)\n",
    "\n",
    "    def forward(self, text_embeddings, vision_embeddings):\n",
    "        text_embeddings = self.text_linear(text_embeddings)\n",
    "        vision_embeddings = self.vision_linear(vision_embeddings)\n",
    "        min_len = min(text_embeddings.size(1), vision_embeddings.size(1))\n",
    "        text_embeddings = text_embeddings[:, :min_len, :]\n",
    "        vision_embeddings = vision_embeddings[:, :min_len, :]\n",
    "        multimodal_embeddings = torch.cat((text_embeddings, vision_embeddings), dim=1)\n",
    "        multimodal_embeddings = self.transformer_encoder(multimodal_embeddings)\n",
    "        lstm_out, _ = self.lstm(multimodal_embeddings)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "def pad_embeddings(embeddings, target_length):\n",
    "    if embeddings.size(1) < target_length:\n",
    "        padding = torch.zeros((embeddings.size(0), target_length - embeddings.size(1), embeddings.size(2)), device=embeddings.device)\n",
    "        embeddings = torch.cat((embeddings, padding), dim=1)\n",
    "    return embeddings\n",
    "\n",
    "def get_embeddings(text_list, image_tensors):\n",
    "    inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "        ModalityType.VISION: image_tensors.to(device)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = imagebind_model(inputs)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    log_file = open(\"log.txt\", \"w\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            summaries, images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            text_embeddings_list = []\n",
    "            vision_embeddings_list = []\n",
    "            for day_summaries, day_images in zip(summaries[0], images[0]):\n",
    "                embeddings = get_embeddings(day_summaries, day_images)\n",
    "                text_embeddings_list.append(embeddings[ModalityType.TEXT])\n",
    "                vision_embeddings_list.append(embeddings[ModalityType.VISION])\n",
    "            \n",
    "            max_len = 60\n",
    "            text_embeddings_list = text_embeddings_list[:max_len]\n",
    "            vision_embeddings_list = vision_embeddings_list[:max_len]\n",
    "\n",
    "            text_embeddings = torch.stack(text_embeddings_list, dim=0)\n",
    "            vision_embeddings = torch.stack(vision_embeddings_list, dim=0)\n",
    "\n",
    "            target_length = max(text_embeddings.size(1), vision_embeddings.size(1))\n",
    "            text_embeddings = pad_embeddings(text_embeddings, target_length)\n",
    "            vision_embeddings = pad_embeddings(vision_embeddings, target_length)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text_embeddings, vision_embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        log_file.write(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\\n')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}')\n",
    "    log_file.close()\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "text_embedding_dim = 1024\n",
    "vision_embedding_dim = 1024\n",
    "common_embedding_dim = 768\n",
    "num_heads = 8\n",
    "\n",
    "model = CrossAttentionFusionLSTM(text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(data_dir, \"multimodal_model.pth\"))\n",
    "\n",
    "print(\"Model training completed and saved!\")\n",
    "\n",
    "# Evaluation Code\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            summaries, images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            text_embeddings_list = []\n",
    "            vision_embeddings_list = []\n",
    "            for day_summaries, day_images in zip(summaries[0], images[0]):\n",
    "                embeddings = get_embeddings(day_summaries, day_images)\n",
    "                text_embeddings_list.append(embeddings[ModalityType.TEXT])\n",
    "                vision_embeddings_list.append(embeddings[ModalityType.VISION])\n",
    "            \n",
    "            max_len = 60\n",
    "            text_embeddings_list = text_embeddings_list[:max_len]\n",
    "            vision_embeddings_list = vision_embeddings_list[:max_len]\n",
    "\n",
    "            text_embeddings = torch.stack(text_embeddings_list, dim=0)\n",
    "            vision_embeddings = torch.stack(vision_embeddings_list, dim=0)\n",
    "\n",
    "            target_length = max(text_embeddings.size(1), vision_embeddings.size(1))\n",
    "            text_embeddings = pad_embeddings(text_embeddings, target_length)\n",
    "            vision_embeddings = pad_embeddings(vision_embeddings, target_length)\n",
    "\n",
    "            outputs = model(text_embeddings, vision_embeddings)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels_max = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels_max).sum().item()\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = correct / total\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    else:\n",
    "        accuracy = 0\n",
    "        print(\"No data to evaluate.\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
