{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files and directories are verified to exist.\n",
      "Raw data samples: 42698\n",
      "After 2 months data samples: 43683\n",
      "Converted post_date to standard format and removed invalid dates.\n",
      "Randomly sampled 426 raw data samples.\n",
      "Filtered after 2 months data to 694 samples.\n",
      "Training raw data samples: 327\n",
      "Testing raw data samples: 99\n",
      "Training after 2 month data samples: 500\n",
      "Testing after 2 month data samples: 194\n",
      "Training after 2 month data samples after cleaning: 498\n",
      "Testing after 2 month data samples after cleaning: 194\n",
      "Number of training windows: 268\n",
      "Number of testing windows: 40\n",
      "Image transformations defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 268/268 [15:08<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed samples: 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 40/40 [02:16<00:00,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed samples: 12\n",
      "Number of training samples: 121\n",
      "Number of test samples: 12\n",
      "Number of batches in train_loader: 121\n",
      "Number of batches in test_loader: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "# Ensure CUDA_LAUNCH_BLOCKING is set for better debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Define file paths\n",
    "data_dir = \"/data1/dxw_data/llm/redbook_final/script_next/\"\n",
    "rawdata_path = os.path.join(data_dir, \"rawdata_20%.csv\")\n",
    "after2monthdata_path = os.path.join(data_dir, \"after2monthdata_20%_with_trend.csv\")\n",
    "image_dir = os.path.join(data_dir, \"data_img_20%\")\n",
    "\n",
    "# Check if files and directories exist\n",
    "assert os.path.exists(rawdata_path), f\"File not found: {rawdata_path}\"\n",
    "assert os.path.exists(after2monthdata_path), f\"File not found: {after2monthdata_path}\"\n",
    "assert os.path.exists(image_dir), f\"Directory not found: {image_dir}\"\n",
    "\n",
    "print(\"All files and directories are verified to exist.\")\n",
    "\n",
    "# Read CSV files\n",
    "rawdata = pd.read_csv(rawdata_path)\n",
    "after2monthdata = pd.read_csv(after2monthdata_path)\n",
    "print(f\"Raw data samples: {len(rawdata)}\")\n",
    "print(f\"After 2 months data samples: {len(after2monthdata)}\")\n",
    "\n",
    "# Convert date columns to standard format\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "rawdata['post_date'] = rawdata['post_date'].apply(parse_date)\n",
    "rawdata = rawdata.dropna(subset=['post_date'])\n",
    "print(\"Converted post_date to standard format and removed invalid dates.\")\n",
    "\n",
    "# Function to randomly sample a subset of the data\n",
    "def get_subset_indices(data, fraction=0.01):\n",
    "    data_size = len(data)\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(fraction * data_size))\n",
    "    return indices[:split]\n",
    "\n",
    "# Random sampling of 1/100th of the data\n",
    "subset_indices = get_subset_indices(rawdata)\n",
    "rawdata = rawdata.iloc[subset_indices]\n",
    "after2monthdata = after2monthdata[after2monthdata['post_id'].isin(rawdata['post_id'])]\n",
    "print(f\"Randomly sampled {len(rawdata)} raw data samples.\")\n",
    "print(f\"Filtered after 2 months data to {len(after2monthdata)} samples.\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_rawdata = rawdata[(rawdata['post_date'].dt.month >= 1) & (rawdata['post_date'].dt.month <= 9)]\n",
    "test_rawdata = rawdata[(rawdata['post_date'].dt.month >= 10) & (rawdata['post_date'].dt.month >= 10)]\n",
    "print(f\"Training raw data samples: {len(train_rawdata)}\")\n",
    "print(f\"Testing raw data samples: {len(test_rawdata)}\")\n",
    "\n",
    "train_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(train_rawdata['post_id'])]\n",
    "test_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(test_rawdata['post_id'])]\n",
    "print(f\"Training after 2 month data samples: {len(train_after2monthdata)}\")\n",
    "print(f\"Testing after 2 month data samples: {len(test_after2monthdata)}\")\n",
    "\n",
    "# Remove non-finite values in the 'trend' column\n",
    "train_after2monthdata = train_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['trend'])\n",
    "test_after2monthdata = test_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['trend'])\n",
    "print(f\"Training after 2 month data samples after cleaning: {len(train_after2monthdata)}\")\n",
    "print(f\"Testing after 2 month data samples after cleaning: {len(test_after2monthdata)}\")\n",
    "\n",
    "# Ensure that the 'trend' column has correct integer labels\n",
    "train_after2monthdata['trend'] = train_after2monthdata['trend'].astype(int)\n",
    "test_after2monthdata['trend'] = test_after2monthdata['trend'].astype(int)\n",
    "\n",
    "# Define a sliding window function\n",
    "def sliding_window(data, window_size=60):\n",
    "    num_windows = len(data) - window_size + 1\n",
    "    return [data[i:i + window_size] for i in range(num_windows)]\n",
    "\n",
    "# Get sliding windows for training and testing data\n",
    "train_windows = sliding_window(train_rawdata)\n",
    "test_windows = sliding_window(test_rawdata)\n",
    "print(f\"Number of training windows: {len(train_windows)}\")\n",
    "print(f\"Number of testing windows: {len(test_windows)}\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, windows, after2monthdata, image_dir, transform=None, max_images=1):\n",
    "        self.windows = windows\n",
    "        self.after2monthdata = after2monthdata\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.max_images = max_images\n",
    "        self.data = self._prepare_data()\n",
    "        print(f\"Number of processed samples: {len(self.data)}\")  # Debug information\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        data = []\n",
    "        for window in tqdm(self.windows, desc=\"Processing data\"):\n",
    "            window_data = []\n",
    "            for _, row in window.iterrows():\n",
    "                poster_id = row['poster_id']\n",
    "                post_id = row['post_id']\n",
    "                image_files = [f for f in os.listdir(self.image_dir) if f\"{poster_id}_{post_id}\" in f]\n",
    "                images = []\n",
    "                for image_file in image_files[:self.max_images]:\n",
    "                    image_path = os.path.join(self.image_dir, image_file)\n",
    "                    try:\n",
    "                        image = Image.open(image_path).convert('RGB')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error opening image {image_path}: {e}\")\n",
    "                        continue\n",
    "                    if self.transform:\n",
    "                        image = self.transform(image)\n",
    "                    images.append(image)\n",
    "                while len(images) < self.max_images:\n",
    "                    images.append(torch.zeros((3, 224, 224)))\n",
    "                if images:\n",
    "                    summary = row['summary']\n",
    "                    numerical_list = [float(row['post_comments']), float(row['post_like']), float(row['post_collect'])]\n",
    "                    window_data.append((summary, images, numerical_list))\n",
    "            if window_data:\n",
    "                last_day_post_id = window.iloc[-1]['post_id']\n",
    "                label_data = self.after2monthdata[self.after2monthdata['post_id'] == last_day_post_id]['trend']\n",
    "                if not label_data.empty:\n",
    "                    label = label_data.values[0]\n",
    "                    one_hot_label = F.one_hot(torch.tensor(label + 1), num_classes=3)  # -1, 0, 1 -> 0, 1, 2\n",
    "                    data.append((window_data, one_hot_label))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window_data, label = self.data[idx]\n",
    "        summaries, images, numerical_lists = zip(*window_data)\n",
    "        images = [torch.stack(image_set).float() for image_set in images]\n",
    "        return summaries, torch.stack(images), torch.tensor(numerical_lists).float(), label.float()\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "print(\"Image transformations defined.\")\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = MultimodalDataset(train_windows, train_after2monthdata, image_dir, transform=transform)\n",
    "test_dataset = MultimodalDataset(test_windows, test_after2monthdata, image_dir, transform=transform)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageBind model loaded and set to evaluation mode.\n",
      "Model architecture defined.\n",
      "Model, criterion, and optimizer initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████| 121/121 [01:51<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5580742075608289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 121/121 [02:11<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.4985420394165457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 121/121 [02:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.48576662624793604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 121/121 [02:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.4791376998971316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 121/121 [02:06<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.46107262255977993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 121/121 [02:04<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.4958052186310784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 121/121 [02:03<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.4697736882843262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 121/121 [02:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.4795233825267839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 121/121 [02:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.47239513288844714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 121/121 [02:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.4655125682388455\n",
      "Model training completed and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 12/12 [00:13<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model components\n",
    "device = \"cuda:4\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load ImageBind model\n",
    "imagebind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind_model.eval()\n",
    "imagebind_model.to(device)\n",
    "print(\"ImageBind model loaded and set to evaluation mode.\")\n",
    "\n",
    "# Define CrossAttentionFusionLSTM model with MLP for numerical features\n",
    "class CrossAttentionFusionLSTM(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads, numerical_feature_dim):\n",
    "        super(CrossAttentionFusionLSTM, self).__init__()\n",
    "        self.text_linear = nn.Linear(text_embedding_dim, common_embedding_dim)\n",
    "        self.vision_linear = nn.Linear(vision_embedding_dim, common_embedding_dim)\n",
    "        self.numerical_mlp = nn.Sequential(\n",
    "            nn.Linear(numerical_feature_dim, common_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(common_embedding_dim, common_embedding_dim)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=common_embedding_dim, nhead=num_heads), num_layers=2)\n",
    "        self.lstm = nn.LSTM(common_embedding_dim, common_embedding_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(common_embedding_dim, 3)\n",
    "\n",
    "    def forward(self, text_embeddings, vision_embeddings, numerical_embeddings):\n",
    "        text_embeddings = self.text_linear(text_embeddings)\n",
    "        vision_embeddings = self.vision_linear(vision_embeddings)\n",
    "        numerical_embeddings = self.numerical_mlp(numerical_embeddings)\n",
    "        min_len = min(text_embeddings.size(1), vision_embeddings.size(1), numerical_embeddings.size(1))\n",
    "        text_embeddings = text_embeddings[:, :min_len, :]\n",
    "        vision_embeddings = vision_embeddings[:, :min_len, :]\n",
    "        numerical_embeddings = numerical_embeddings[:, :min_len, :]\n",
    "        multimodal_embeddings = torch.cat((text_embeddings, vision_embeddings, numerical_embeddings), dim=1)\n",
    "        multimodal_embeddings = self.transformer_encoder(multimodal_embeddings)\n",
    "        lstm_out, _ = self.lstm(multimodal_embeddings)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "print(\"Model architecture defined.\")\n",
    "\n",
    "# Padding function\n",
    "def pad_embeddings(embeddings, target_length):\n",
    "    if embeddings.size(1) < target_length:\n",
    "        padding = torch.zeros((embeddings.size(0), target_length - embeddings.size(1), embeddings.size(2)), device=embeddings.device)\n",
    "        embeddings = torch.cat((embeddings, padding), dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Get embeddings function\n",
    "def get_embeddings(text_list, image_tensors):\n",
    "    inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "        ModalityType.VISION: image_tensors.to(device)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = imagebind_model(inputs)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Train model function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    log_file = open(\"log.txt\", \"w\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            summaries, images, numerical_lists, labels = batch\n",
    "            images, labels, numerical_lists = images.to(device), labels.to(device), numerical_lists.to(device)\n",
    "            text_embeddings_list = []\n",
    "            vision_embeddings_list = []\n",
    "            for day_summaries, day_images in zip(summaries[0], images[0]):\n",
    "                embeddings = get_embeddings(day_summaries, day_images)\n",
    "                text_embeddings_list.append(embeddings[ModalityType.TEXT])\n",
    "                vision_embeddings_list.append(embeddings[ModalityType.VISION])\n",
    "            \n",
    "            max_len = 60\n",
    "            text_embeddings_list = text_embeddings_list[:max_len]\n",
    "            vision_embeddings_list = vision_embeddings_list[:max_len]\n",
    "            numerical_lists = numerical_lists[:, :max_len, :]\n",
    "\n",
    "            text_embeddings = torch.stack(text_embeddings_list, dim=0)\n",
    "            vision_embeddings = torch.stack(vision_embeddings_list, dim=0)\n",
    "\n",
    "            target_length = max(text_embeddings.size(1), vision_embeddings.size(1), numerical_lists.size(1))\n",
    "            text_embeddings = pad_embeddings(text_embeddings, target_length)\n",
    "            vision_embeddings = pad_embeddings(vision_embeddings, target_length)\n",
    "            numerical_lists = pad_embeddings(numerical_lists, target_length)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text_embeddings, vision_embeddings, numerical_lists)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        log_file.write(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\\n')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}')\n",
    "    log_file.close()\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "text_embedding_dim = 1024\n",
    "vision_embedding_dim = 1024\n",
    "common_embedding_dim = 768\n",
    "num_heads = 8\n",
    "numerical_feature_dim = 3\n",
    "\n",
    "model = CrossAttentionFusionLSTM(text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads, numerical_feature_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Model, criterion, and optimizer initialized.\")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(data_dir, \"multimodal_model.pth\"))\n",
    "print(\"Model training completed and saved!\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            summaries, images, numerical_lists, labels = batch\n",
    "            images, labels, numerical_lists = images.to(device), labels.to(device), numerical_lists.to(device)\n",
    "            text_embeddings_list = []\n",
    "            vision_embeddings_list = []\n",
    "            for day_summaries, day_images in zip(summaries[0], images[0]):\n",
    "                embeddings = get_embeddings(day_summaries, day_images)\n",
    "                text_embeddings_list.append(embeddings[ModalityType.TEXT])\n",
    "                vision_embeddings_list.append(embeddings[ModalityType.VISION])\n",
    "            \n",
    "            max_len = 60\n",
    "            text_embeddings_list = text_embeddings_list[:max_len]\n",
    "            vision_embeddings_list = vision_embeddings_list[:max_len]\n",
    "            numerical_lists = numerical_lists[:, :max_len, :]\n",
    "\n",
    "            text_embeddings = torch.stack(text_embeddings_list, dim=0)\n",
    "            vision_embeddings = torch.stack(vision_embeddings_list, dim=0)\n",
    "\n",
    "            target_length = max(text_embeddings.size(1), vision_embeddings.size(1), numerical_lists.size(1))\n",
    "            text_embeddings = pad_embeddings(text_embeddings, target_length)\n",
    "            vision_embeddings = pad_embeddings(vision_embeddings, target_length)\n",
    "            numerical_lists = pad_embeddings(numerical_lists, target_length)\n",
    "\n",
    "            outputs = model(text_embeddings, vision_embeddings, numerical_lists)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels_max = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels_max).sum().item()\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = correct / total\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    else:\n",
    "        accuracy = 0\n",
    "        print(\"No data to evaluate.\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
