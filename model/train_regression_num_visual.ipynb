{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3367278/1568302966.py:32: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rawdata = pd.read_csv(rawdata_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw data samples: 426\n",
      "Total after 2 month data samples: 592\n",
      "Training raw data samples: 354\n",
      "Testing raw data samples: 72\n",
      "Training after 2 month data samples: 481\n",
      "Testing after 2 month data samples: 111\n",
      "Training after 2 month data samples after cleaning: 479\n",
      "Testing after 2 month data samples after cleaning: 111\n",
      "Number of training windows: 295\n",
      "Number of testing windows: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 295/295 [16:56<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed samples: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 13/13 [00:47<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed samples: 9\n",
      "Number of training samples: 143\n",
      "Number of test samples: 9\n",
      "Number of batches in train_loader: 143\n",
      "Number of batches in test_loader: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure CUDA_LAUNCH_BLOCKING is set for better debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Define file paths\n",
    "data_dir = \"/data1/dxw_data/llm/redbook_final/script_next/\"\n",
    "rawdata_path = os.path.join(data_dir, \"rawdata_20%.csv\")\n",
    "after2monthdata_path = os.path.join(data_dir, \"after2monthdata_20%_with_trend.csv\")\n",
    "image_dir = os.path.join(data_dir, \"data_img_20%\")\n",
    "\n",
    "# Check if files exist\n",
    "assert os.path.exists(rawdata_path), f\"File not found: {rawdata_path}\"\n",
    "assert os.path.exists(after2monthdata_path), f\"File not found: {after2monthdata_path}\"\n",
    "assert os.path.exists(image_dir), f\"Directory not found: {image_dir}\"\n",
    "\n",
    "# Read CSV files\n",
    "rawdata = pd.read_csv(rawdata_path)\n",
    "after2monthdata = pd.read_csv(after2monthdata_path)\n",
    "\n",
    "# Convert date columns to standard format\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "rawdata['post_date'] = rawdata['post_date'].apply(parse_date)\n",
    "rawdata = rawdata.dropna(subset=['post_date'])\n",
    "\n",
    "# Only use 1/10th of the data\n",
    "def get_subset_indices(data, fraction=0.01):\n",
    "    data_size = len(data)\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(fraction * data_size))\n",
    "    return indices[:split]\n",
    "\n",
    "# Random sampling of 1/10th of the data\n",
    "subset_indices = get_subset_indices(rawdata)\n",
    "rawdata = rawdata.iloc[subset_indices]\n",
    "after2monthdata = after2monthdata[after2monthdata['post_id'].isin(rawdata['post_id'])]\n",
    "\n",
    "print(f\"Total raw data samples: {len(rawdata)}\")\n",
    "print(f\"Total after 2 month data samples: {len(after2monthdata)}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_rawdata = rawdata[(rawdata['post_date'].dt.month >= 1) & (rawdata['post_date'].dt.month <= 9)]\n",
    "test_rawdata = rawdata[(rawdata['post_date'].dt.month >= 10) & (rawdata['post_date'].dt.month >= 10)]\n",
    "\n",
    "train_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(train_rawdata['post_id'])]\n",
    "test_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(test_rawdata['post_id'])]\n",
    "\n",
    "print(f\"Training raw data samples: {len(train_rawdata)}\")\n",
    "print(f\"Testing raw data samples: {len(test_rawdata)}\")\n",
    "print(f\"Training after 2 month data samples: {len(train_after2monthdata)}\")\n",
    "print(f\"Testing after 2 month data samples: {len(test_after2monthdata)}\")\n",
    "\n",
    "# Remove non-finite values in the 'proportion' column\n",
    "train_after2monthdata = train_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['proportion'])\n",
    "test_after2monthdata = test_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['proportion'])\n",
    "\n",
    "print(f\"Training after 2 month data samples after cleaning: {len(train_after2monthdata)}\")\n",
    "print(f\"Testing after 2 month data samples after cleaning: {len(test_after2monthdata)}\")\n",
    "\n",
    "# Define a sliding window function\n",
    "def sliding_window(data, window_size=60):\n",
    "    num_windows = len(data) - window_size + 1\n",
    "    return [data[i:i + window_size] for i in range(num_windows)]\n",
    "\n",
    "# Get sliding windows for training and testing data\n",
    "train_windows = sliding_window(train_rawdata)\n",
    "test_windows = sliding_window(test_rawdata)\n",
    "\n",
    "print(f\"Number of training windows: {len(train_windows)}\")\n",
    "print(f\"Number of testing windows: {len(test_windows)}\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, windows, after2monthdata, image_dir, transform=None, max_images=1):\n",
    "        self.windows = windows\n",
    "        self.after2monthdata = after2monthdata\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.max_images = max_images\n",
    "        self.data = self._prepare_data()\n",
    "        print(f\"Number of processed samples: {len(self.data)}\")  # Debug information\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        data = []\n",
    "        for window in tqdm(self.windows, desc=\"Processing data\"):\n",
    "            window_data = []\n",
    "            for _, row in window.iterrows():\n",
    "                poster_id = row['poster_id']\n",
    "                post_id = row['post_id']\n",
    "                numerical_list = [float(row['post_comments']), float(row['post_like']), float(row['post_collect'])]\n",
    "                image_files = [f for f in os.listdir(self.image_dir) if f\"{poster_id}_{post_id}\" in f]\n",
    "                images = []\n",
    "                for image_file in image_files[:self.max_images]:\n",
    "                    image_path = os.path.join(self.image_dir, image_file)\n",
    "                    try:\n",
    "                        image = Image.open(image_path).convert('RGB')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error opening image {image_path}: {e}\")\n",
    "                        continue\n",
    "                    if self.transform:\n",
    "                        image = self.transform(image)\n",
    "                    images.append(image)\n",
    "                while len(images) < self.max_images:\n",
    "                    images.append(torch.zeros((3, 224, 224)))\n",
    "                if images:\n",
    "                    summary = row['summary']\n",
    "                    window_data.append((summary, images, numerical_list))\n",
    "            if window_data:\n",
    "                last_day_post_id = window.iloc[-1]['post_id']\n",
    "                label_data = self.after2monthdata[self.after2monthdata['post_id'] == last_day_post_id]['proportion']\n",
    "                if not label_data.empty:\n",
    "                    label = label_data.values[0]\n",
    "                    data.append((window_data, label))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window_data, label = self.data[idx]\n",
    "        summaries, images, numerical_lists = zip(*window_data)\n",
    "        images = [torch.stack(image_set).float() for image_set in images]\n",
    "        numerical_lists = torch.tensor(numerical_lists, dtype=torch.float32)\n",
    "        return summaries, torch.stack(images), numerical_lists, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = MultimodalDataset(train_windows, train_after2monthdata, image_dir, transform=transform)\n",
    "test_dataset = MultimodalDataset(test_windows, test_after2monthdata, image_dir, transform=transform)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageBind model loaded and set to evaluation mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:   0%|          | 0/143 [00:00<?, ?it/s]/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training Epoch 1/10: 100%|██████████| 143/143 [01:53<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.06931708769122925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 143/143 [01:53<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.0013596198687284241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 143/143 [01:56<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.0010496387026859072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 143/143 [01:55<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0011664689175648153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 143/143 [01:55<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0007553383740587937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 143/143 [01:54<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0010643122333760062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 143/143 [01:55<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0009871462980241207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 143/143 [01:53<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0006436407464656124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 143/143 [01:53<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0006675773468884399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 143/143 [01:53<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.000899616655415643\n",
      "Model training completed and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 9/9 [00:04<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 0.00033924097693827093\n",
      "Evaluation Loss: 0.00033924097693827093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load imagebind model\n",
    "imagebind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind_model.eval()\n",
    "imagebind_model.to(device)\n",
    "print(\"ImageBind model loaded and set to evaluation mode.\")\n",
    "\n",
    "class CrossAttentionFusionLSTM(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, vision_embedding_dim, common_embedding_dim, numerical_feature_dim, num_heads):\n",
    "        super(CrossAttentionFusionLSTM, self).__init__()\n",
    "        self.text_linear = nn.Linear(text_embedding_dim, common_embedding_dim)\n",
    "        self.vision_linear = nn.Linear(vision_embedding_dim, common_embedding_dim)\n",
    "        self.numerical_mlp = nn.Sequential(\n",
    "            nn.Linear(numerical_feature_dim, common_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(common_embedding_dim, common_embedding_dim)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=common_embedding_dim, nhead=num_heads), num_layers=2)\n",
    "        self.lstm = nn.LSTM(common_embedding_dim, common_embedding_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(common_embedding_dim, 1)\n",
    "\n",
    "    def forward(self, text_embeddings, vision_embeddings, numerical_features):\n",
    "        text_embeddings = self.text_linear(text_embeddings)\n",
    "        vision_embeddings = self.vision_linear(vision_embeddings)\n",
    "        numerical_embeddings = self.numerical_mlp(numerical_features)\n",
    "        min_len = min(text_embeddings.size(1), vision_embeddings.size(1), numerical_embeddings.size(1))\n",
    "        text_embeddings = text_embeddings[:, :min_len, :]\n",
    "        vision_embeddings = vision_embeddings[:, :min_len, :]\n",
    "        numerical_embeddings = numerical_embeddings[:, :min_len, :]\n",
    "        multimodal_embeddings = torch.cat((text_embeddings, vision_embeddings, numerical_embeddings), dim=1)\n",
    "        multimodal_embeddings = self.transformer_encoder(multimodal_embeddings)\n",
    "        lstm_out, _ = self.lstm(multimodal_embeddings)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "def pad_embeddings(embeddings, target_length):\n",
    "    if embeddings.size(1) < target_length:\n",
    "        padding = torch.zeros((embeddings.size(0), target_length - embeddings.size(1), embeddings.size(2)), device=embeddings.device)\n",
    "        embeddings = torch.cat((embeddings, padding), dim=1)\n",
    "    return embeddings\n",
    "\n",
    "def get_embeddings(text_list, image_tensors):\n",
    "    inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "        ModalityType.VISION: image_tensors.to(device)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = imagebind_model(inputs)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    log_file = open(\"log.txt\", \"w\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            summaries, images, numerical_lists, labels = batch\n",
    "            images, numerical_lists, labels = images.to(device), numerical_lists.to(device), labels.to(device)\n",
    "            text_embeddings_list = []\n",
    "            vision_embeddings_list = []\n",
    "            for day_summaries, day_images in zip(summaries[0], images[0]):\n",
    "                embeddings = get_embeddings(day_summaries, day_images)\n",
    "                text_embeddings_list.append(embeddings[ModalityType.TEXT])\n",
    "                vision_embeddings_list.append(embeddings[ModalityType.VISION])\n",
    "            \n",
    "            max_len = 60\n",
    "            text_embeddings_list = text_embeddings_list[:max_len]\n",
    "            vision_embeddings_list = vision_embeddings_list[:max_len]\n",
    "            numerical_lists = numerical_lists[:, :max_len, :]\n",
    "\n",
    "            text_embeddings = torch.stack(text_embeddings_list, dim=0)\n",
    "            vision_embeddings = torch.stack(vision_embeddings_list, dim=0)\n",
    "\n",
    "            target_length = max(text_embeddings.size(1), vision_embeddings.size(1), numerical_lists.size(1))\n",
    "            text_embeddings = pad_embeddings(text_embeddings, target_length)\n",
    "            vision_embeddings = pad_embeddings(vision_embeddings, target_length)\n",
    "            numerical_lists = pad_embeddings(numerical_lists, target_length)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text_embeddings, vision_embeddings, numerical_lists)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        log_file.write(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\\n')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}')\n",
    "    log_file.close()\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "text_embedding_dim = 1024\n",
    "vision_embedding_dim = 1024\n",
    "common_embedding_dim = 768\n",
    "numerical_feature_dim = 3\n",
    "num_heads = 8\n",
    "\n",
    "model = CrossAttentionFusionLSTM(text_embedding_dim, vision_embedding_dim, common_embedding_dim, numerical_feature_dim, num_heads).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(data_dir, \"multimodal_regression_model.pth\"))\n",
    "\n",
    "print(\"Model training completed and saved!\")\n",
    "\n",
    "# Evaluation Code\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            summaries, images, numerical_lists, labels = batch\n",
    "            images, numerical_lists, labels = images.to(device), numerical_lists.to(device), labels.to(device)\n",
    "            text_embeddings_list = []\n",
    "            vision_embeddings_list = []\n",
    "            for day_summaries, day_images in zip(summaries[0], images[0]):\n",
    "                embeddings = get_embeddings(day_summaries, day_images)\n",
    "                text_embeddings_list.append(embeddings[ModalityType.TEXT])\n",
    "                vision_embeddings_list.append(embeddings[ModalityType.VISION])\n",
    "            \n",
    "            max_len = 60\n",
    "            text_embeddings_list = text_embeddings_list[:max_len]\n",
    "            vision_embeddings_list = vision_embeddings_list[:max_len]\n",
    "            numerical_lists = numerical_lists[:, :max_len, :]\n",
    "\n",
    "            text_embeddings = torch.stack(text_embeddings_list, dim=0)\n",
    "            vision_embeddings = torch.stack(vision_embeddings_list, dim=0)\n",
    "\n",
    "            target_length = max(text_embeddings.size(1), vision_embeddings.size(1), numerical_lists.size(1))\n",
    "            text_embeddings = pad_embeddings(text_embeddings, target_length)\n",
    "            vision_embeddings = pad_embeddings(vision_embeddings, target_length)\n",
    "            numerical_lists = pad_embeddings(numerical_lists, target_length)\n",
    "\n",
    "            outputs = model(text_embeddings, vision_embeddings, numerical_lists)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f'Evaluation Loss: {avg_loss}')\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = evaluate_model(model, test_loader)\n",
    "\n",
    "print(f\"Evaluation Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
