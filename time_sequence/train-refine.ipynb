{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------更新后的思路---------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from gensim.models import KeyedVectors\n",
    "from PIL import Image\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Paths\n",
    "original_image_path = '/data1/dxw_data/llm/redbook/1000/data2'\n",
    "segmented_image_path = '/data1/dxw_data/llm/redbook/combine_final'\n",
    "text_feature_path = '/data1/dxw_data/llm/redbook/time_sequence/captions_with_hotness_and_time.json'\n",
    "sequence_data_path = '/data1/dxw_data/llm/redbook-refine/train_data.json'\n",
    "\n",
    "# Load pretrained ResNet\n",
    "resnet = resnet50(pretrained=False)\n",
    "resnet.load_state_dict(torch.load('/data1/dxw_data/llm/resnet/resnet50-19c8e357.pth'))\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the classification layer\n",
    "resnet.eval()\n",
    "\n",
    "# Load pretrained Word2Vec model\n",
    "word2vec_path = '/data1/dxw_data/llm/word2vec/GoogleNews-vectors-negative300.bin.gz' #! 可以修改为中文大模型large-chinese-word2vec\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "# Function to extract image features\n",
    "def extract_image_features(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = resnet(image).squeeze().numpy()\n",
    "    return features\n",
    "\n",
    "# Function to extract text features\n",
    "def extract_text_features(caption):\n",
    "    words = caption.split()\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            vector = word2vec_model[word]\n",
    "            word_vectors.append(vector)\n",
    "    if not word_vectors:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Define the dataset class\n",
    "class HotnessDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Define the LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the output of the last time step\n",
    "        out = self.fc(lstm_out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Load sequence data to get the 序号\n",
    "with open(sequence_data_path, 'r') as f:\n",
    "    sequence_data = json.load(f)\n",
    "\n",
    "sequence_map = {int(item['序号']): item for item in sequence_data}\n",
    "\n",
    "# Load JSON file with text features\n",
    "with open(text_feature_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare dataset\n",
    "combined_features = []\n",
    "labels = []\n",
    "times = []\n",
    "\n",
    "for item in data:\n",
    "    seq_num = int(item['image'])  # Convert seq_num to integer\n",
    "    # print(seq_num)\n",
    "    text = item['caption']\n",
    "    # print(text)\n",
    "    print(sequence_map)\n",
    "    segmented_image_path='/data1/dxw_data/llm/redbook/combine_final'\n",
    "    if seq_num in sequence_map:\n",
    "        image_path = os.path.join(original_image_path, f\"{seq_num}.png\")\n",
    "        segmented_image_path = os.path.join(segmented_image_path, f\"{seq_num}.png\")\n",
    "\n",
    "        # Debugging statements to check paths and existence\n",
    "        print(f\"Checking paths for seq_num: {seq_num}\")\n",
    "        print(f\"Original image path: {image_path}\")\n",
    "        print(f\"Segmented image path: {segmented_image_path}\")\n",
    "\n",
    "        if os.path.exists(image_path) and os.path.exists(segmented_image_path):\n",
    "            print(f\"Paths exist for seq_num: {seq_num}\")\n",
    "\n",
    "            # Extract features\n",
    "            image_features = extract_image_features(image_path)\n",
    "            mask_features = extract_image_features(segmented_image_path)\n",
    "            text_features = extract_text_features(text)\n",
    "\n",
    "            # Flatten image features to 1D if necessary\n",
    "            image_features = image_features.flatten()\n",
    "            mask_features = mask_features.flatten()\n",
    "\n",
    "            print(\"image_features.shape\", image_features.shape)\n",
    "            print(\"mask_features.shape\", mask_features.shape)\n",
    "            print(\"text_features.shape\", text_features.shape)\n",
    "\n",
    "            # Combine features\n",
    "            combined_feature = np.hstack((mask_features, image_features, text_features))\n",
    "            combined_features.append(combined_feature)\n",
    "\n",
    "            labels.append(sequence_map[seq_num]['本产品当前火爆'])\n",
    "            times.append(sequence_map[seq_num]['天数'])\n",
    "        else:\n",
    "            print(f\"Paths do not exist for seq_num: {seq_num}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "combined_features = np.array(combined_features)\n",
    "labels = np.array(labels)\n",
    "times = np.array(times)\n",
    "\n",
    "print(f\"Total combined features: {len(combined_features)}\")\n",
    "print(f\"Total labels: {len(labels)}\")\n",
    "print(f\"Total times: {len(times)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.3704\n",
      "Epoch [2/20], Loss: 0.4480\n",
      "Epoch [3/20], Loss: 0.7093\n",
      "Epoch [4/20], Loss: 0.4657\n",
      "Epoch [5/20], Loss: 0.5305\n",
      "Epoch [6/20], Loss: 0.5723\n",
      "Epoch [7/20], Loss: 0.7054\n",
      "Epoch [8/20], Loss: 0.5726\n",
      "Epoch [9/20], Loss: 0.6155\n",
      "Epoch [10/20], Loss: 0.4105\n",
      "Epoch [11/20], Loss: 0.7482\n",
      "Epoch [12/20], Loss: 0.6135\n",
      "Epoch [13/20], Loss: 0.5729\n",
      "Epoch [14/20], Loss: 0.4557\n",
      "Epoch [15/20], Loss: 0.4462\n",
      "Epoch [16/20], Loss: 0.4221\n",
      "Epoch [17/20], Loss: 0.6632\n",
      "Epoch [18/20], Loss: 0.4899\n",
      "Epoch [19/20], Loss: 0.6165\n",
      "Epoch [20/20], Loss: 0.4476\n",
      "Test Accuracy: 75.38%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to encode time information\n",
    "def encode_time(times, max_time):\n",
    "    times = np.array(times)\n",
    "    sin_time = np.sin(2 * np.pi * times / max_time)\n",
    "    cos_time = np.cos(2 * np.pi * times / max_time)\n",
    "    return np.vstack((sin_time, cos_time)).T\n",
    "\n",
    "encoded_times = encode_time(times, max_time=100)\n",
    "\n",
    "# Split data into training and testing\n",
    "train_indices = np.where(times <= 80)[0]\n",
    "test_indices = np.where(times > 80)[0]\n",
    "\n",
    "time_train = encoded_times[train_indices]\n",
    "time_test = encoded_times[test_indices]\n",
    "\n",
    "# Ensure combined_features is 2D before concatenating\n",
    "if combined_features.ndim == 1:\n",
    "    combined_features = combined_features.reshape(-1, 1)\n",
    "\n",
    "# Concatenate time information with features\n",
    "X_train = np.hstack((combined_features[train_indices], time_train))\n",
    "X_test = np.hstack((combined_features[test_indices], time_test))\n",
    "y_train = labels[train_indices]\n",
    "y_test = labels[test_indices]\n",
    "\n",
    "# Function to create sliding windows\n",
    "def create_sliding_windows(X, y, window_size):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - window_size):\n",
    "        window = X[i:i + window_size]\n",
    "        label = y[i + window_size]\n",
    "        features.append(window)\n",
    "        labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "# Create sliding windows for training and testing\n",
    "X_train, y_train = create_sliding_windows(X_train, y_train, window_size)\n",
    "X_test, y_test = create_sliding_windows(X_test, y_test, window_size)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = HotnessDataset(X_train, y_train)\n",
    "test_dataset = HotnessDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
